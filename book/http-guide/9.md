# 09. 웹 로봇

웹 로봇은 사람과의 상호작용 없이 연속된 웹 트랜잭션들을 자동으로 수행하는 소프트웨어 프로그램이다. 많은 로봇이 웹 사이트에서 다른 웹 사이트로 떠돌아다니면서, 콘텐츠를 가져오고, 하이퍼링크를 따라가고, 그들이 발견한 데이터를 처리한다.

## 크롤러와 크롤링

웹 크롤러는 먼저 웹페이지를 한 개 가져오고, 그 다음 그 페이지가 가리키는 모든 웹페이지를 가져오고, 다시 그 페이지들이 가리키는 모든 웹페이지들을 가져오는, 이러한 일을 재귀적으로 반복하는 방식으로 웹을 순회하는 로봇이다.

인터넷 검색엔진은 웹을 돌아다니면서 그들이 만나는 모든 문서를 끌어오기 위해 크롤러를 사용한다. 이 문서들은 나중에 처리되어 검색 가능한 데이터베이스로 만들어지는데, 이는 사용자들이 특정 단어를 포함한 문서를 찾을 수 있게 해준다.

## 로봇 차단하기

로봇이 그들에게 맞지 않는 장소에 들어오지 않도록 하고 웹 마스터에게 로봇의 동작을 더 잘 제어할 수 있는 메커니즘을 제공하는 단순하고 자발적인 기법이 제안되었다. 이 표준은 Robots Exclusion Standard라고 이름 지어졌지만, 로봇의 접근을 제어하는 정보를 저장하는 파일의 이름을 따서 종종 그냥 `robots.txt`라고 불린다.

웹 사이트의 어떤 URL을 방문하기 전에, 그 웹 사이트에 robots.txt 파일이 존재한다면 로봇은 반드시 그 파일을 가져와서 처리해야 한다. 호스트 명과 포트번호에 의해 정의되는 어떤 웹 사이트가 있을 때, 그 사이트 전체에 대한 robots.txt 파일은 단 하나만이 존재한다.

로봇은 HTTP GET 메서드를 이용해 robots.txt 리소스를 가져온다. 존재한다면 서버는 그 파일을 text/plain 본문으로 반환한다. 만약 서버가 404로 응답한다면 로봇은 그 서버는 로봇의 접근을 제한하지 않는 것으로 간주하고 어떤 파일이든 요청하게 될 것이다.

```text
# 이 robots.txt 파일은 slurp과 webcrawler가 우리 사이트의 공개된
# 영역을 크롤링하는 것을 허락한다. 그러나 다른 로봇은 안된다

# User-Agent: <robot-name>
User-Agent: slurp
User-Agent: webcrawler
Disallow: /private

User-Agent: *
Disallow: 
```

## 검색엔진

웹 로봇을 가장 광범위하게 사용하는 것은 인터넷 검색엔진이다. 웹 크롤러들은 검색엔진에게 웹에 존재하는 문서들을 가져다 주어서, 검색엔진이 어떤 문서에 어떤 단어들이 존재하는지에 대한 색인을 생성할 수 있게 한다.

